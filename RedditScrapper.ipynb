{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b556b5b",
   "metadata": {},
   "source": [
    "Goal: To scrap Data from Reddit handle r/dataengineering after scrolling till certain no_of_days\n",
    "\n",
    "And then find order the data in upvotes\n",
    "\n",
    "then load the data in Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5720608",
   "metadata": {},
   "source": [
    "## First step: To scrape data for last 7 days I need to scroll down the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf569fb",
   "metadata": {},
   "source": [
    "I need to install Selenium to scroll data in webpage\n",
    "\n",
    "For webdrivers: we need Selenium and a webdriver\n",
    "\n",
    "pip install selenium\n",
    "\n",
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0333af",
   "metadata": {},
   "source": [
    "First import and install chrome webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e44b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21325/57819732.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "# For Chrome Driver\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "\n",
    "# For finding and waiting for elements\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fa5ea290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e937e3f",
   "metadata": {},
   "source": [
    "## Fixing the URL\n",
    "## Change the subbreddit handle you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84fe42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_domain = \"https://www.reddit.com\"\n",
    "# subreddit_url = \"/r/dataengineering/\"\n",
    "subreddit_url = '/r/Python/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6932406",
   "metadata": {},
   "source": [
    "Now pass the URL you need to open in the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "413a5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url_domain + subreddit_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604b743",
   "metadata": {},
   "source": [
    "The below code will scroll the websit until certains old post arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a046a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 3)\n",
    "\n",
    "pending_list = {3, 4}\n",
    "found = False\n",
    "count = 0\n",
    "scroll_from = 0\n",
    "scroll_to = 3000\n",
    "while not found:\n",
    "    driver.execute_script(f\"window.scrollTo({scroll_from},{scroll_from + scroll_to})\")\n",
    "    scroll_from += scroll_to\n",
    "#     print(pending_list, scroll_from)\n",
    "    found = not pending_list\n",
    "    try:\n",
    "        done_set = set()\n",
    "        for k in pending_list:\n",
    "            find_elem = wait.until(EC.presence_of_element_located((By.XPATH, f\"//*[contains(text(), '{k} days ago')]\")))\n",
    "            if find_elem: done_set.add(k)\n",
    "        pending_list = pending_list - done_set\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a458b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrapped_data.txt', 'w') as f:\n",
    "    f.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a556a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e6aad",
   "metadata": {},
   "source": [
    "## Second Step: Now we need to scrap the data from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c26ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af267ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrapped_data.txt', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9ad77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_elem = soup.find(\"div\", attrs={\"tabindex\":\"0\"}).parent\n",
    "elem = parent_elem.findChildren(\"div\", recursive=False)[3].findChildren(\"div\", recursive=False)\n",
    "\n",
    "# data = soup.find(\"div\", class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "data = parent_elem.findChildren(\"div\", recursive=False)[3].select(\"div[style]\", recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a671cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for e in elem:\n",
    "    link = upvote = None\n",
    "    link_elem = e.find(\"a\", {\"data-click-id\": \"body\"})\n",
    "    if link_elem:\n",
    "        link = link_elem.get('href')\n",
    "    button = e.find(\"button\")\n",
    "    if button:\n",
    "        upvote = button.parent.find(\"div\").text\n",
    "    span = e.find('span', {\"data-click-id\": \"timestamp\"})\n",
    "    title = e.find('h3')\n",
    "    if link and upvote and span and title:\n",
    "#         pass\n",
    "        data_dict = {'link': link,\n",
    "                    'upvote': upvote,\n",
    "                    'time': span.text,\n",
    "                    'title': title.text}\n",
    "        data.append(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df39e7",
   "metadata": {},
   "source": [
    "## Step3: Use Pandas to Tranform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3766948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4be12104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be29e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upvote'] = df['upvote'].replace('Vote', 0)\n",
    "# to convert upvote to Integer\n",
    "df['upvote'] = df['upvote'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fb94fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To categorize time as D-Days, H-Hours, M-Minutes, S-Seconds\n",
    "def categorize_time(x):\n",
    "    if 'day' in x:\n",
    "        x = x.split()\n",
    "        return f'{x[0]} D'\n",
    "    elif 'hour' in x:\n",
    "        x = x.split()\n",
    "        return f'{x[0]} H'\n",
    "    elif 'minute' in x:\n",
    "        x = x.split()\n",
    "        return f'{x[0]} M'\n",
    "    elif 'second' in x:\n",
    "        x = x.split()\n",
    "        return f'{x[0]} S'\n",
    "    else:\n",
    "        return None\n",
    "df['time_category'] = df['time'].apply(categorize_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34f27ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Categorizing time, we are reducing the time if difference in days, if diffence in time like H, M, S then default as today\n",
    "def time_diff(x):\n",
    "    t, c = x.split()\n",
    "    t = int(t)\n",
    "    if c in ('H', 'M', 'S'):\n",
    "        return pd.Timestamp.now().floor('d')\n",
    "    elif c in ('D'):\n",
    "        return pd.Timestamp.now().floor('d') - pd.Timedelta(days = t)\n",
    "    else:\n",
    "        return None\n",
    "df['post_date'] = df['time_category'].apply(time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20659a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the domain to construct workable link\n",
    "df['link'] = df['link'].apply(lambda x: url_domain + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce67363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_loaded = df[['title', 'upvote', 'link', 'post_date']].sort_values(by='upvote', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c03dd",
   "metadata": {},
   "source": [
    "## Step3: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c8c9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 77 entries, 61 to 20\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   title      77 non-null     object        \n",
      " 1   upvote     77 non-null     int64         \n",
      " 2   link       77 non-null     object        \n",
      " 3   post_date  77 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_to_be_loaded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55f9f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_loaded.to_csv('reddit_upvote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "797175e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9c06b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to an existing database\n",
    "    conn = psycopg2.connect(user=\"python_psql\",\n",
    "                                  password=\"password\",\n",
    "                                  host=\"127.0.0.1\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"reddit_scrapper\")\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table if not exists\n",
    "    query = '''create table if not exists scrapped_data(\n",
    "            id serial primary key,\n",
    "            title varchar(300),\n",
    "            upvote int,\n",
    "            link varchar(300),\n",
    "            post_date date\n",
    "        )'''\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Convert date-time to string to upload in sql\n",
    "    # df_to_be_loaded['post_date'] = df_to_be_loaded['post_date'].dt.strftime('%d-%m-%Y')\n",
    "    data_to_be_inserted = df_to_be_loaded.to_numpy().tolist()\n",
    "    query = \"INSERT INTO scrapped_data(title, upvote, link, post_date) VALUES(%s, %s, %s, %s)\"\n",
    "    cursor.executemany(query,data_to_be_inserted)\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b680eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
